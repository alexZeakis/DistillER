{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9963369963369964,
  "eval_steps": 500,
  "global_step": 408,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007326007326007326,
      "grad_norm": 5.3952765464782715,
      "learning_rate": 0.0,
      "loss": 1.567,
      "step": 1
    },
    {
      "epoch": 0.014652014652014652,
      "grad_norm": 16.705917358398438,
      "learning_rate": 4e-05,
      "loss": 1.6627,
      "step": 2
    },
    {
      "epoch": 0.02197802197802198,
      "grad_norm": 10.032648086547852,
      "learning_rate": 8e-05,
      "loss": 1.5527,
      "step": 3
    },
    {
      "epoch": 0.029304029304029304,
      "grad_norm": 7.816343784332275,
      "learning_rate": 0.00012,
      "loss": 1.6287,
      "step": 4
    },
    {
      "epoch": 0.03663003663003663,
      "grad_norm": 17.821592330932617,
      "learning_rate": 0.00016,
      "loss": 1.233,
      "step": 5
    },
    {
      "epoch": 0.04395604395604396,
      "grad_norm": 3.984259605407715,
      "learning_rate": 0.0002,
      "loss": 1.1624,
      "step": 6
    },
    {
      "epoch": 0.05128205128205128,
      "grad_norm": 2.9369866847991943,
      "learning_rate": 0.00019950372208436725,
      "loss": 0.9013,
      "step": 7
    },
    {
      "epoch": 0.05860805860805861,
      "grad_norm": 5.300571918487549,
      "learning_rate": 0.0001990074441687345,
      "loss": 0.9961,
      "step": 8
    },
    {
      "epoch": 0.06593406593406594,
      "grad_norm": 2.1343460083007812,
      "learning_rate": 0.00019851116625310176,
      "loss": 0.9979,
      "step": 9
    },
    {
      "epoch": 0.07326007326007326,
      "grad_norm": 59671.98828125,
      "learning_rate": 0.000198014888337469,
      "loss": 0.8183,
      "step": 10
    },
    {
      "epoch": 0.08058608058608059,
      "grad_norm": 1.728489637374878,
      "learning_rate": 0.00019751861042183625,
      "loss": 0.747,
      "step": 11
    },
    {
      "epoch": 0.08791208791208792,
      "grad_norm": 1.336139440536499,
      "learning_rate": 0.00019702233250620346,
      "loss": 0.9745,
      "step": 12
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 1.987553596496582,
      "learning_rate": 0.0001965260545905707,
      "loss": 0.6524,
      "step": 13
    },
    {
      "epoch": 0.10256410256410256,
      "grad_norm": 1.3746660947799683,
      "learning_rate": 0.00019602977667493798,
      "loss": 0.6597,
      "step": 14
    },
    {
      "epoch": 0.10989010989010989,
      "grad_norm": 6.738719463348389,
      "learning_rate": 0.00019553349875930522,
      "loss": 0.9568,
      "step": 15
    },
    {
      "epoch": 0.11721611721611722,
      "grad_norm": 2.0607850551605225,
      "learning_rate": 0.00019503722084367246,
      "loss": 0.7479,
      "step": 16
    },
    {
      "epoch": 0.12454212454212454,
      "grad_norm": 0.9884408712387085,
      "learning_rate": 0.0001945409429280397,
      "loss": 0.6691,
      "step": 17
    },
    {
      "epoch": 0.13186813186813187,
      "grad_norm": 1.069689393043518,
      "learning_rate": 0.00019404466501240697,
      "loss": 0.8856,
      "step": 18
    },
    {
      "epoch": 0.1391941391941392,
      "grad_norm": 1.0459916591644287,
      "learning_rate": 0.00019354838709677422,
      "loss": 0.7358,
      "step": 19
    },
    {
      "epoch": 0.14652014652014653,
      "grad_norm": 1.148960828781128,
      "learning_rate": 0.00019305210918114146,
      "loss": 1.0258,
      "step": 20
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 1.0609517097473145,
      "learning_rate": 0.0001925558312655087,
      "loss": 1.1772,
      "step": 21
    },
    {
      "epoch": 0.16117216117216118,
      "grad_norm": 1.0973188877105713,
      "learning_rate": 0.00019205955334987594,
      "loss": 0.8706,
      "step": 22
    },
    {
      "epoch": 0.1684981684981685,
      "grad_norm": 1.015074610710144,
      "learning_rate": 0.00019156327543424318,
      "loss": 0.7143,
      "step": 23
    },
    {
      "epoch": 0.17582417582417584,
      "grad_norm": 0.9522140622138977,
      "learning_rate": 0.00019106699751861043,
      "loss": 0.8171,
      "step": 24
    },
    {
      "epoch": 0.18315018315018314,
      "grad_norm": 0.9436737298965454,
      "learning_rate": 0.00019057071960297767,
      "loss": 0.7951,
      "step": 25
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.8430465459823608,
      "learning_rate": 0.0001900744416873449,
      "loss": 0.6719,
      "step": 26
    },
    {
      "epoch": 0.1978021978021978,
      "grad_norm": 0.8574405908584595,
      "learning_rate": 0.00018957816377171215,
      "loss": 0.6985,
      "step": 27
    },
    {
      "epoch": 0.20512820512820512,
      "grad_norm": 0.89170902967453,
      "learning_rate": 0.00018908188585607942,
      "loss": 0.825,
      "step": 28
    },
    {
      "epoch": 0.21245421245421245,
      "grad_norm": 0.7725244164466858,
      "learning_rate": 0.00018858560794044667,
      "loss": 0.4469,
      "step": 29
    },
    {
      "epoch": 0.21978021978021978,
      "grad_norm": 0.8519586324691772,
      "learning_rate": 0.0001880893300248139,
      "loss": 0.6898,
      "step": 30
    },
    {
      "epoch": 0.2271062271062271,
      "grad_norm": 0.9726985692977905,
      "learning_rate": 0.00018759305210918115,
      "loss": 0.8396,
      "step": 31
    },
    {
      "epoch": 0.23443223443223443,
      "grad_norm": 1.5038173198699951,
      "learning_rate": 0.0001870967741935484,
      "loss": 0.872,
      "step": 32
    },
    {
      "epoch": 0.24175824175824176,
      "grad_norm": 8.528291702270508,
      "learning_rate": 0.00018660049627791564,
      "loss": 0.7318,
      "step": 33
    },
    {
      "epoch": 0.2490842490842491,
      "grad_norm": 1.4854309558868408,
      "learning_rate": 0.00018610421836228288,
      "loss": 0.5459,
      "step": 34
    },
    {
      "epoch": 0.2564102564102564,
      "grad_norm": 0.843451738357544,
      "learning_rate": 0.00018560794044665012,
      "loss": 0.4465,
      "step": 35
    },
    {
      "epoch": 0.26373626373626374,
      "grad_norm": 1.0487420558929443,
      "learning_rate": 0.00018511166253101736,
      "loss": 0.7024,
      "step": 36
    },
    {
      "epoch": 0.27106227106227104,
      "grad_norm": 1.0476816892623901,
      "learning_rate": 0.00018461538461538463,
      "loss": 0.7832,
      "step": 37
    },
    {
      "epoch": 0.2783882783882784,
      "grad_norm": 1.017574667930603,
      "learning_rate": 0.00018411910669975187,
      "loss": 0.8571,
      "step": 38
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.8325408101081848,
      "learning_rate": 0.00018362282878411912,
      "loss": 0.6709,
      "step": 39
    },
    {
      "epoch": 0.29304029304029305,
      "grad_norm": 0.7630586624145508,
      "learning_rate": 0.00018312655086848636,
      "loss": 0.8293,
      "step": 40
    },
    {
      "epoch": 0.30036630036630035,
      "grad_norm": 35101.71875,
      "learning_rate": 0.0001826302729528536,
      "loss": 0.7156,
      "step": 41
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 2.733818292617798,
      "learning_rate": 0.00018213399503722087,
      "loss": 0.573,
      "step": 42
    },
    {
      "epoch": 0.315018315018315,
      "grad_norm": 0.9438515305519104,
      "learning_rate": 0.00018163771712158811,
      "loss": 0.5462,
      "step": 43
    },
    {
      "epoch": 0.32234432234432236,
      "grad_norm": 1.0765546560287476,
      "learning_rate": 0.00018114143920595536,
      "loss": 0.635,
      "step": 44
    },
    {
      "epoch": 0.32967032967032966,
      "grad_norm": 0.8145139217376709,
      "learning_rate": 0.00018064516129032257,
      "loss": 0.7076,
      "step": 45
    },
    {
      "epoch": 0.336996336996337,
      "grad_norm": 0.8006020784378052,
      "learning_rate": 0.00018014888337468981,
      "loss": 0.7124,
      "step": 46
    },
    {
      "epoch": 0.3443223443223443,
      "grad_norm": 0.9524920582771301,
      "learning_rate": 0.00017965260545905708,
      "loss": 0.5997,
      "step": 47
    },
    {
      "epoch": 0.3516483516483517,
      "grad_norm": 0.8809866905212402,
      "learning_rate": 0.00017915632754342433,
      "loss": 0.8558,
      "step": 48
    },
    {
      "epoch": 0.358974358974359,
      "grad_norm": 0.8909018039703369,
      "learning_rate": 0.00017866004962779157,
      "loss": 0.6155,
      "step": 49
    },
    {
      "epoch": 0.3663003663003663,
      "grad_norm": 1.0216851234436035,
      "learning_rate": 0.0001781637717121588,
      "loss": 0.7553,
      "step": 50
    },
    {
      "epoch": 0.37362637362637363,
      "grad_norm": 0.820587694644928,
      "learning_rate": 0.00017766749379652605,
      "loss": 0.438,
      "step": 51
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.8424001336097717,
      "learning_rate": 0.00017717121588089332,
      "loss": 0.5366,
      "step": 52
    },
    {
      "epoch": 0.3882783882783883,
      "grad_norm": 1.0677087306976318,
      "learning_rate": 0.00017667493796526057,
      "loss": 0.7902,
      "step": 53
    },
    {
      "epoch": 0.3956043956043956,
      "grad_norm": 17.64421844482422,
      "learning_rate": 0.0001761786600496278,
      "loss": 0.7988,
      "step": 54
    },
    {
      "epoch": 0.40293040293040294,
      "grad_norm": 0.9572391510009766,
      "learning_rate": 0.00017568238213399505,
      "loss": 0.8217,
      "step": 55
    },
    {
      "epoch": 0.41025641025641024,
      "grad_norm": 0.799979031085968,
      "learning_rate": 0.0001751861042183623,
      "loss": 0.4527,
      "step": 56
    },
    {
      "epoch": 0.4175824175824176,
      "grad_norm": 0.9130256175994873,
      "learning_rate": 0.00017468982630272953,
      "loss": 0.5525,
      "step": 57
    },
    {
      "epoch": 0.4249084249084249,
      "grad_norm": 6.233805179595947,
      "learning_rate": 0.00017419354838709678,
      "loss": 0.7373,
      "step": 58
    },
    {
      "epoch": 0.43223443223443225,
      "grad_norm": 0.908291220664978,
      "learning_rate": 0.00017369727047146402,
      "loss": 0.7809,
      "step": 59
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 0.9413212537765503,
      "learning_rate": 0.00017320099255583126,
      "loss": 0.663,
      "step": 60
    },
    {
      "epoch": 0.4468864468864469,
      "grad_norm": 0.8298022150993347,
      "learning_rate": 0.00017270471464019853,
      "loss": 0.7544,
      "step": 61
    },
    {
      "epoch": 0.4542124542124542,
      "grad_norm": 0.7610460519790649,
      "learning_rate": 0.00017220843672456577,
      "loss": 0.5458,
      "step": 62
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 0.9111348986625671,
      "learning_rate": 0.00017171215880893302,
      "loss": 0.5951,
      "step": 63
    },
    {
      "epoch": 0.46886446886446886,
      "grad_norm": 0.7200788855552673,
      "learning_rate": 0.00017121588089330026,
      "loss": 0.3935,
      "step": 64
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.8801583051681519,
      "learning_rate": 0.0001707196029776675,
      "loss": 0.7304,
      "step": 65
    },
    {
      "epoch": 0.4835164835164835,
      "grad_norm": 0.8100855350494385,
      "learning_rate": 0.00017022332506203474,
      "loss": 0.5234,
      "step": 66
    },
    {
      "epoch": 0.4908424908424908,
      "grad_norm": 0.8275557160377502,
      "learning_rate": 0.00016972704714640199,
      "loss": 0.6721,
      "step": 67
    },
    {
      "epoch": 0.4981684981684982,
      "grad_norm": 0.7419783473014832,
      "learning_rate": 0.00016923076923076923,
      "loss": 0.4403,
      "step": 68
    },
    {
      "epoch": 0.5054945054945055,
      "grad_norm": 1768.3646240234375,
      "learning_rate": 0.00016873449131513647,
      "loss": 0.5428,
      "step": 69
    },
    {
      "epoch": 0.5128205128205128,
      "grad_norm": 0.9968379735946655,
      "learning_rate": 0.0001682382133995037,
      "loss": 0.5342,
      "step": 70
    },
    {
      "epoch": 0.5201465201465202,
      "grad_norm": 0.8156571388244629,
      "learning_rate": 0.00016774193548387098,
      "loss": 0.7054,
      "step": 71
    },
    {
      "epoch": 0.5274725274725275,
      "grad_norm": 0.8861629962921143,
      "learning_rate": 0.00016724565756823823,
      "loss": 0.6405,
      "step": 72
    },
    {
      "epoch": 0.5347985347985348,
      "grad_norm": 0.8405093550682068,
      "learning_rate": 0.00016674937965260547,
      "loss": 0.5125,
      "step": 73
    },
    {
      "epoch": 0.5421245421245421,
      "grad_norm": 609.820068359375,
      "learning_rate": 0.0001662531017369727,
      "loss": 0.6416,
      "step": 74
    },
    {
      "epoch": 0.5494505494505495,
      "grad_norm": 0.6782053112983704,
      "learning_rate": 0.00016575682382133998,
      "loss": 0.5592,
      "step": 75
    },
    {
      "epoch": 0.5567765567765568,
      "grad_norm": 0.7456544041633606,
      "learning_rate": 0.00016526054590570722,
      "loss": 0.5831,
      "step": 76
    },
    {
      "epoch": 0.5641025641025641,
      "grad_norm": 204.7499542236328,
      "learning_rate": 0.00016476426799007444,
      "loss": 0.44,
      "step": 77
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.2700064182281494,
      "learning_rate": 0.00016426799007444168,
      "loss": 0.6355,
      "step": 78
    },
    {
      "epoch": 0.5787545787545788,
      "grad_norm": 0.9155325889587402,
      "learning_rate": 0.00016377171215880892,
      "loss": 0.7075,
      "step": 79
    },
    {
      "epoch": 0.5860805860805861,
      "grad_norm": 0.839946448802948,
      "learning_rate": 0.0001632754342431762,
      "loss": 0.7011,
      "step": 80
    },
    {
      "epoch": 0.5934065934065934,
      "grad_norm": 0.7950314879417419,
      "learning_rate": 0.00016277915632754343,
      "loss": 0.5824,
      "step": 81
    },
    {
      "epoch": 0.6007326007326007,
      "grad_norm": 0.9509228467941284,
      "learning_rate": 0.00016228287841191068,
      "loss": 0.8156,
      "step": 82
    },
    {
      "epoch": 0.608058608058608,
      "grad_norm": 0.8248741626739502,
      "learning_rate": 0.00016178660049627792,
      "loss": 0.4678,
      "step": 83
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 0.9461254477500916,
      "learning_rate": 0.00016129032258064516,
      "loss": 0.4387,
      "step": 84
    },
    {
      "epoch": 0.6227106227106227,
      "grad_norm": 0.7717621326446533,
      "learning_rate": 0.00016079404466501243,
      "loss": 0.5478,
      "step": 85
    },
    {
      "epoch": 0.63003663003663,
      "grad_norm": 0.9474591016769409,
      "learning_rate": 0.00016029776674937967,
      "loss": 0.768,
      "step": 86
    },
    {
      "epoch": 0.6373626373626373,
      "grad_norm": 0.8102626800537109,
      "learning_rate": 0.00015980148883374692,
      "loss": 0.5596,
      "step": 87
    },
    {
      "epoch": 0.6446886446886447,
      "grad_norm": 0.8219464421272278,
      "learning_rate": 0.00015930521091811416,
      "loss": 0.6658,
      "step": 88
    },
    {
      "epoch": 0.652014652014652,
      "grad_norm": 0.9733723402023315,
      "learning_rate": 0.0001588089330024814,
      "loss": 0.7384,
      "step": 89
    },
    {
      "epoch": 0.6593406593406593,
      "grad_norm": 0.7197275161743164,
      "learning_rate": 0.00015831265508684864,
      "loss": 0.6428,
      "step": 90
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.8172913193702698,
      "learning_rate": 0.00015781637717121589,
      "loss": 0.6846,
      "step": 91
    },
    {
      "epoch": 0.673992673992674,
      "grad_norm": 0.7408816814422607,
      "learning_rate": 0.00015732009925558313,
      "loss": 0.4557,
      "step": 92
    },
    {
      "epoch": 0.6813186813186813,
      "grad_norm": 0.7620984315872192,
      "learning_rate": 0.00015682382133995037,
      "loss": 0.5571,
      "step": 93
    },
    {
      "epoch": 0.6886446886446886,
      "grad_norm": 0.6570783257484436,
      "learning_rate": 0.00015632754342431764,
      "loss": 0.5054,
      "step": 94
    },
    {
      "epoch": 0.6959706959706959,
      "grad_norm": 0.9352030754089355,
      "learning_rate": 0.00015583126550868488,
      "loss": 0.7717,
      "step": 95
    },
    {
      "epoch": 0.7032967032967034,
      "grad_norm": 0.7630221247673035,
      "learning_rate": 0.00015533498759305212,
      "loss": 0.6869,
      "step": 96
    },
    {
      "epoch": 0.7106227106227107,
      "grad_norm": 0.7239670157432556,
      "learning_rate": 0.00015483870967741937,
      "loss": 0.5416,
      "step": 97
    },
    {
      "epoch": 0.717948717948718,
      "grad_norm": 0.9497008323669434,
      "learning_rate": 0.0001543424317617866,
      "loss": 0.5363,
      "step": 98
    },
    {
      "epoch": 0.7252747252747253,
      "grad_norm": 1.0529431104660034,
      "learning_rate": 0.00015384615384615385,
      "loss": 0.6706,
      "step": 99
    },
    {
      "epoch": 0.7326007326007326,
      "grad_norm": 0.6253942847251892,
      "learning_rate": 0.0001533498759305211,
      "loss": 0.4552,
      "step": 100
    },
    {
      "epoch": 0.73992673992674,
      "grad_norm": 0.868138313293457,
      "learning_rate": 0.00015285359801488834,
      "loss": 0.6644,
      "step": 101
    },
    {
      "epoch": 0.7472527472527473,
      "grad_norm": 0.764462411403656,
      "learning_rate": 0.00015235732009925558,
      "loss": 0.5494,
      "step": 102
    },
    {
      "epoch": 0.7545787545787546,
      "grad_norm": 0.8874145746231079,
      "learning_rate": 0.00015186104218362282,
      "loss": 0.7556,
      "step": 103
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 28.027816772460938,
      "learning_rate": 0.0001513647642679901,
      "loss": 0.9897,
      "step": 104
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 0.9122427701950073,
      "learning_rate": 0.00015086848635235733,
      "loss": 0.6412,
      "step": 105
    },
    {
      "epoch": 0.7765567765567766,
      "grad_norm": 0.8419275283813477,
      "learning_rate": 0.00015037220843672458,
      "loss": 0.5223,
      "step": 106
    },
    {
      "epoch": 0.7838827838827839,
      "grad_norm": 0.6669573187828064,
      "learning_rate": 0.00014987593052109182,
      "loss": 0.522,
      "step": 107
    },
    {
      "epoch": 0.7912087912087912,
      "grad_norm": 0.9701963663101196,
      "learning_rate": 0.0001493796526054591,
      "loss": 0.4861,
      "step": 108
    },
    {
      "epoch": 0.7985347985347986,
      "grad_norm": 0.7156497836112976,
      "learning_rate": 0.0001488833746898263,
      "loss": 0.483,
      "step": 109
    },
    {
      "epoch": 0.8058608058608059,
      "grad_norm": 0.7774137258529663,
      "learning_rate": 0.00014838709677419355,
      "loss": 0.4314,
      "step": 110
    },
    {
      "epoch": 0.8131868131868132,
      "grad_norm": 0.6387885808944702,
      "learning_rate": 0.0001478908188585608,
      "loss": 0.4122,
      "step": 111
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 0.7134367823600769,
      "learning_rate": 0.00014739454094292803,
      "loss": 0.7111,
      "step": 112
    },
    {
      "epoch": 0.8278388278388278,
      "grad_norm": 0.7955202460289001,
      "learning_rate": 0.0001468982630272953,
      "loss": 0.3916,
      "step": 113
    },
    {
      "epoch": 0.8351648351648352,
      "grad_norm": 0.7508803009986877,
      "learning_rate": 0.00014640198511166254,
      "loss": 0.5301,
      "step": 114
    },
    {
      "epoch": 0.8424908424908425,
      "grad_norm": 1996.5091552734375,
      "learning_rate": 0.00014590570719602978,
      "loss": 0.5147,
      "step": 115
    },
    {
      "epoch": 0.8498168498168498,
      "grad_norm": 10.753631591796875,
      "learning_rate": 0.00014540942928039703,
      "loss": 0.4394,
      "step": 116
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.8409938216209412,
      "learning_rate": 0.00014491315136476427,
      "loss": 0.4476,
      "step": 117
    },
    {
      "epoch": 0.8644688644688645,
      "grad_norm": 0.7543916702270508,
      "learning_rate": 0.00014441687344913154,
      "loss": 0.5196,
      "step": 118
    },
    {
      "epoch": 0.8717948717948718,
      "grad_norm": 0.8086082935333252,
      "learning_rate": 0.00014392059553349878,
      "loss": 0.456,
      "step": 119
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 0.7249455451965332,
      "learning_rate": 0.00014342431761786602,
      "loss": 0.5402,
      "step": 120
    },
    {
      "epoch": 0.8864468864468864,
      "grad_norm": 0.7103566527366638,
      "learning_rate": 0.00014292803970223324,
      "loss": 0.4348,
      "step": 121
    },
    {
      "epoch": 0.8937728937728938,
      "grad_norm": 0.7127115726470947,
      "learning_rate": 0.00014243176178660048,
      "loss": 0.503,
      "step": 122
    },
    {
      "epoch": 0.9010989010989011,
      "grad_norm": 0.6610776782035828,
      "learning_rate": 0.00014193548387096775,
      "loss": 0.5889,
      "step": 123
    },
    {
      "epoch": 0.9084249084249084,
      "grad_norm": 0.723640501499176,
      "learning_rate": 0.000141439205955335,
      "loss": 0.6097,
      "step": 124
    },
    {
      "epoch": 0.9157509157509157,
      "grad_norm": 0.5676260590553284,
      "learning_rate": 0.00014094292803970224,
      "loss": 0.3044,
      "step": 125
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 0.8069524765014648,
      "learning_rate": 0.00014044665012406948,
      "loss": 0.5843,
      "step": 126
    },
    {
      "epoch": 0.9304029304029304,
      "grad_norm": 0.6865924596786499,
      "learning_rate": 0.00013995037220843675,
      "loss": 0.579,
      "step": 127
    },
    {
      "epoch": 0.9377289377289377,
      "grad_norm": 0.6212936043739319,
      "learning_rate": 0.000139454094292804,
      "loss": 0.4868,
      "step": 128
    },
    {
      "epoch": 0.945054945054945,
      "grad_norm": 0.8203988075256348,
      "learning_rate": 0.00013895781637717123,
      "loss": 0.583,
      "step": 129
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.7289758920669556,
      "learning_rate": 0.00013846153846153847,
      "loss": 0.557,
      "step": 130
    },
    {
      "epoch": 0.9597069597069597,
      "grad_norm": 0.6336014270782471,
      "learning_rate": 0.00013796526054590572,
      "loss": 0.5495,
      "step": 131
    },
    {
      "epoch": 0.967032967032967,
      "grad_norm": 0.7283380031585693,
      "learning_rate": 0.00013746898263027296,
      "loss": 0.4786,
      "step": 132
    },
    {
      "epoch": 0.9743589743589743,
      "grad_norm": 0.6226428151130676,
      "learning_rate": 0.0001369727047146402,
      "loss": 0.5957,
      "step": 133
    },
    {
      "epoch": 0.9816849816849816,
      "grad_norm": 0.692146360874176,
      "learning_rate": 0.00013647642679900744,
      "loss": 0.4835,
      "step": 134
    },
    {
      "epoch": 0.989010989010989,
      "grad_norm": 0.5817660689353943,
      "learning_rate": 0.0001359801488833747,
      "loss": 0.4629,
      "step": 135
    },
    {
      "epoch": 0.9963369963369964,
      "grad_norm": 0.6456589698791504,
      "learning_rate": 0.00013548387096774193,
      "loss": 0.5811,
      "step": 136
    },
    {
      "epoch": 1.0073260073260073,
      "grad_norm": 1.7485015392303467,
      "learning_rate": 0.0001349875930521092,
      "loss": 1.015,
      "step": 137
    },
    {
      "epoch": 1.0146520146520146,
      "grad_norm": 0.5328080654144287,
      "learning_rate": 0.00013449131513647644,
      "loss": 0.3176,
      "step": 138
    },
    {
      "epoch": 1.021978021978022,
      "grad_norm": 0.7191315293312073,
      "learning_rate": 0.00013399503722084368,
      "loss": 0.375,
      "step": 139
    },
    {
      "epoch": 1.0293040293040292,
      "grad_norm": 0.48183372616767883,
      "learning_rate": 0.00013349875930521093,
      "loss": 0.3245,
      "step": 140
    },
    {
      "epoch": 1.0366300366300367,
      "grad_norm": 0.5170077681541443,
      "learning_rate": 0.00013300248138957817,
      "loss": 0.3014,
      "step": 141
    },
    {
      "epoch": 1.043956043956044,
      "grad_norm": 0.4860333204269409,
      "learning_rate": 0.0001325062034739454,
      "loss": 0.2713,
      "step": 142
    },
    {
      "epoch": 1.0512820512820513,
      "grad_norm": 0.5999641418457031,
      "learning_rate": 0.00013200992555831265,
      "loss": 0.2796,
      "step": 143
    },
    {
      "epoch": 1.0586080586080586,
      "grad_norm": 0.6239500045776367,
      "learning_rate": 0.0001315136476426799,
      "loss": 0.4161,
      "step": 144
    },
    {
      "epoch": 1.065934065934066,
      "grad_norm": 0.5800139904022217,
      "learning_rate": 0.00013101736972704714,
      "loss": 0.3547,
      "step": 145
    },
    {
      "epoch": 1.0732600732600732,
      "grad_norm": 0.5446736216545105,
      "learning_rate": 0.0001305210918114144,
      "loss": 0.2775,
      "step": 146
    },
    {
      "epoch": 1.0805860805860805,
      "grad_norm": 0.5201054811477661,
      "learning_rate": 0.00013002481389578165,
      "loss": 0.275,
      "step": 147
    },
    {
      "epoch": 1.0879120879120878,
      "grad_norm": 0.6088817715644836,
      "learning_rate": 0.0001295285359801489,
      "loss": 0.3999,
      "step": 148
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.6228632926940918,
      "learning_rate": 0.00012903225806451613,
      "loss": 0.3903,
      "step": 149
    },
    {
      "epoch": 1.1025641025641026,
      "grad_norm": 0.5595029592514038,
      "learning_rate": 0.00012853598014888338,
      "loss": 0.3106,
      "step": 150
    },
    {
      "epoch": 1.10989010989011,
      "grad_norm": 0.5787191390991211,
      "learning_rate": 0.00012803970223325065,
      "loss": 0.3033,
      "step": 151
    },
    {
      "epoch": 1.1172161172161172,
      "grad_norm": 0.5936163067817688,
      "learning_rate": 0.0001275434243176179,
      "loss": 0.3075,
      "step": 152
    },
    {
      "epoch": 1.1245421245421245,
      "grad_norm": 0.6197664141654968,
      "learning_rate": 0.00012704714640198513,
      "loss": 0.3636,
      "step": 153
    },
    {
      "epoch": 1.1318681318681318,
      "grad_norm": 0.6351653933525085,
      "learning_rate": 0.00012655086848635235,
      "loss": 0.3373,
      "step": 154
    },
    {
      "epoch": 1.1391941391941391,
      "grad_norm": 0.5621187090873718,
      "learning_rate": 0.0001260545905707196,
      "loss": 0.2384,
      "step": 155
    },
    {
      "epoch": 1.1465201465201464,
      "grad_norm": 0.5548404455184937,
      "learning_rate": 0.00012555831265508686,
      "loss": 0.2764,
      "step": 156
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 0.557553231716156,
      "learning_rate": 0.0001250620347394541,
      "loss": 0.3249,
      "step": 157
    },
    {
      "epoch": 1.1611721611721613,
      "grad_norm": 0.48054179549217224,
      "learning_rate": 0.00012456575682382134,
      "loss": 0.2478,
      "step": 158
    },
    {
      "epoch": 1.1684981684981686,
      "grad_norm": 0.603994607925415,
      "learning_rate": 0.00012406947890818859,
      "loss": 0.3229,
      "step": 159
    },
    {
      "epoch": 1.1758241758241759,
      "grad_norm": 0.6893327832221985,
      "learning_rate": 0.00012357320099255583,
      "loss": 0.2269,
      "step": 160
    },
    {
      "epoch": 1.1831501831501832,
      "grad_norm": 0.5581715703010559,
      "learning_rate": 0.0001230769230769231,
      "loss": 0.3398,
      "step": 161
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.4776078760623932,
      "learning_rate": 0.00012258064516129034,
      "loss": 0.2444,
      "step": 162
    },
    {
      "epoch": 1.1978021978021978,
      "grad_norm": 0.5752129554748535,
      "learning_rate": 0.00012208436724565758,
      "loss": 0.264,
      "step": 163
    },
    {
      "epoch": 1.205128205128205,
      "grad_norm": 0.6678305268287659,
      "learning_rate": 0.00012158808933002481,
      "loss": 0.3473,
      "step": 164
    },
    {
      "epoch": 1.2124542124542124,
      "grad_norm": 0.5663561820983887,
      "learning_rate": 0.00012109181141439208,
      "loss": 0.3122,
      "step": 165
    },
    {
      "epoch": 1.2197802197802199,
      "grad_norm": 0.6769530773162842,
      "learning_rate": 0.00012059553349875932,
      "loss": 0.3208,
      "step": 166
    },
    {
      "epoch": 1.2271062271062272,
      "grad_norm": 0.5835347771644592,
      "learning_rate": 0.00012009925558312655,
      "loss": 0.3762,
      "step": 167
    },
    {
      "epoch": 1.2344322344322345,
      "grad_norm": 0.6029554605484009,
      "learning_rate": 0.0001196029776674938,
      "loss": 0.3976,
      "step": 168
    },
    {
      "epoch": 1.2417582417582418,
      "grad_norm": 0.6451686024665833,
      "learning_rate": 0.00011910669975186104,
      "loss": 0.3534,
      "step": 169
    },
    {
      "epoch": 1.249084249084249,
      "grad_norm": 0.533592700958252,
      "learning_rate": 0.0001186104218362283,
      "loss": 0.2844,
      "step": 170
    },
    {
      "epoch": 1.2564102564102564,
      "grad_norm": 0.5733036994934082,
      "learning_rate": 0.00011811414392059555,
      "loss": 0.4178,
      "step": 171
    },
    {
      "epoch": 1.2637362637362637,
      "grad_norm": 0.5796741843223572,
      "learning_rate": 0.00011761786600496279,
      "loss": 0.2329,
      "step": 172
    },
    {
      "epoch": 1.271062271062271,
      "grad_norm": 0.644531786441803,
      "learning_rate": 0.00011712158808933002,
      "loss": 0.2817,
      "step": 173
    },
    {
      "epoch": 1.2783882783882783,
      "grad_norm": 0.6171554327011108,
      "learning_rate": 0.00011662531017369726,
      "loss": 0.3974,
      "step": 174
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.5241490006446838,
      "learning_rate": 0.00011612903225806453,
      "loss": 0.3063,
      "step": 175
    },
    {
      "epoch": 1.293040293040293,
      "grad_norm": 0.5617344975471497,
      "learning_rate": 0.00011563275434243177,
      "loss": 0.3579,
      "step": 176
    },
    {
      "epoch": 1.3003663003663004,
      "grad_norm": 0.5627633929252625,
      "learning_rate": 0.00011513647642679902,
      "loss": 0.3543,
      "step": 177
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.5501402020454407,
      "learning_rate": 0.00011464019851116626,
      "loss": 0.2826,
      "step": 178
    },
    {
      "epoch": 1.315018315018315,
      "grad_norm": 0.5565730333328247,
      "learning_rate": 0.00011414392059553349,
      "loss": 0.3059,
      "step": 179
    },
    {
      "epoch": 1.3223443223443223,
      "grad_norm": 0.550687849521637,
      "learning_rate": 0.00011364764267990076,
      "loss": 0.3805,
      "step": 180
    },
    {
      "epoch": 1.3296703296703296,
      "grad_norm": 0.5609687566757202,
      "learning_rate": 0.000113151364764268,
      "loss": 0.2861,
      "step": 181
    },
    {
      "epoch": 1.3369963369963371,
      "grad_norm": 0.5031130313873291,
      "learning_rate": 0.00011265508684863524,
      "loss": 0.2127,
      "step": 182
    },
    {
      "epoch": 1.3443223443223444,
      "grad_norm": 0.5450643301010132,
      "learning_rate": 0.00011215880893300248,
      "loss": 0.338,
      "step": 183
    },
    {
      "epoch": 1.3516483516483517,
      "grad_norm": 0.5570720434188843,
      "learning_rate": 0.00011166253101736974,
      "loss": 0.3581,
      "step": 184
    },
    {
      "epoch": 1.358974358974359,
      "grad_norm": 0.5274785757064819,
      "learning_rate": 0.00011116625310173698,
      "loss": 0.2893,
      "step": 185
    },
    {
      "epoch": 1.3663003663003663,
      "grad_norm": 0.5406515002250671,
      "learning_rate": 0.00011066997518610423,
      "loss": 0.2503,
      "step": 186
    },
    {
      "epoch": 1.3736263736263736,
      "grad_norm": 0.4955659508705139,
      "learning_rate": 0.00011017369727047147,
      "loss": 0.2804,
      "step": 187
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.529962420463562,
      "learning_rate": 0.00010967741935483871,
      "loss": 0.2875,
      "step": 188
    },
    {
      "epoch": 1.3882783882783882,
      "grad_norm": 0.8937066197395325,
      "learning_rate": 0.00010918114143920597,
      "loss": 0.3355,
      "step": 189
    },
    {
      "epoch": 1.3956043956043955,
      "grad_norm": 2500.431884765625,
      "learning_rate": 0.00010868486352357321,
      "loss": 0.1913,
      "step": 190
    },
    {
      "epoch": 1.4029304029304028,
      "grad_norm": 0.5820066332817078,
      "learning_rate": 0.00010818858560794045,
      "loss": 0.3484,
      "step": 191
    },
    {
      "epoch": 1.4102564102564101,
      "grad_norm": 0.5389013290405273,
      "learning_rate": 0.0001076923076923077,
      "loss": 0.3784,
      "step": 192
    },
    {
      "epoch": 1.4175824175824177,
      "grad_norm": 0.5890028476715088,
      "learning_rate": 0.00010719602977667494,
      "loss": 0.3778,
      "step": 193
    },
    {
      "epoch": 1.424908424908425,
      "grad_norm": 0.5268267393112183,
      "learning_rate": 0.00010669975186104219,
      "loss": 0.2579,
      "step": 194
    },
    {
      "epoch": 1.4322344322344323,
      "grad_norm": 240.25103759765625,
      "learning_rate": 0.00010620347394540943,
      "loss": 0.239,
      "step": 195
    },
    {
      "epoch": 1.4395604395604396,
      "grad_norm": 0.649267852306366,
      "learning_rate": 0.00010570719602977668,
      "loss": 0.3291,
      "step": 196
    },
    {
      "epoch": 1.4468864468864469,
      "grad_norm": 0.4687546491622925,
      "learning_rate": 0.00010521091811414392,
      "loss": 0.3018,
      "step": 197
    },
    {
      "epoch": 1.4542124542124542,
      "grad_norm": 0.5691336393356323,
      "learning_rate": 0.00010471464019851116,
      "loss": 0.2965,
      "step": 198
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 0.6309665441513062,
      "learning_rate": 0.00010421836228287842,
      "loss": 0.2947,
      "step": 199
    },
    {
      "epoch": 1.468864468864469,
      "grad_norm": 0.5486873388290405,
      "learning_rate": 0.00010372208436724566,
      "loss": 0.3195,
      "step": 200
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.6563999652862549,
      "learning_rate": 0.0001032258064516129,
      "loss": 0.3054,
      "step": 201
    },
    {
      "epoch": 1.4835164835164836,
      "grad_norm": 0.5017310976982117,
      "learning_rate": 0.00010272952853598014,
      "loss": 0.309,
      "step": 202
    },
    {
      "epoch": 1.4908424908424909,
      "grad_norm": 0.5435535907745361,
      "learning_rate": 0.00010223325062034741,
      "loss": 0.2698,
      "step": 203
    },
    {
      "epoch": 1.4981684981684982,
      "grad_norm": 0.5023258924484253,
      "learning_rate": 0.00010173697270471466,
      "loss": 0.2247,
      "step": 204
    },
    {
      "epoch": 1.5054945054945055,
      "grad_norm": 0.639228880405426,
      "learning_rate": 0.00010124069478908189,
      "loss": 0.375,
      "step": 205
    },
    {
      "epoch": 1.5128205128205128,
      "grad_norm": 0.5784434080123901,
      "learning_rate": 0.00010074441687344913,
      "loss": 0.3332,
      "step": 206
    },
    {
      "epoch": 1.52014652014652,
      "grad_norm": 0.5636203289031982,
      "learning_rate": 0.00010024813895781637,
      "loss": 0.3413,
      "step": 207
    },
    {
      "epoch": 1.5274725274725274,
      "grad_norm": 0.5589780807495117,
      "learning_rate": 9.975186104218363e-05,
      "loss": 0.3136,
      "step": 208
    },
    {
      "epoch": 1.5347985347985347,
      "grad_norm": 0.48430314660072327,
      "learning_rate": 9.925558312655088e-05,
      "loss": 0.2735,
      "step": 209
    },
    {
      "epoch": 1.542124542124542,
      "grad_norm": 0.6468044519424438,
      "learning_rate": 9.875930521091812e-05,
      "loss": 0.3358,
      "step": 210
    },
    {
      "epoch": 1.5494505494505495,
      "grad_norm": 0.3898848295211792,
      "learning_rate": 9.826302729528535e-05,
      "loss": 0.1984,
      "step": 211
    },
    {
      "epoch": 1.5567765567765568,
      "grad_norm": 0.5384168028831482,
      "learning_rate": 9.776674937965261e-05,
      "loss": 0.2877,
      "step": 212
    },
    {
      "epoch": 1.564102564102564,
      "grad_norm": 0.5141520500183105,
      "learning_rate": 9.727047146401985e-05,
      "loss": 0.2744,
      "step": 213
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.6160030961036682,
      "learning_rate": 9.677419354838711e-05,
      "loss": 0.3138,
      "step": 214
    },
    {
      "epoch": 1.578754578754579,
      "grad_norm": 0.49516919255256653,
      "learning_rate": 9.627791563275435e-05,
      "loss": 0.2546,
      "step": 215
    },
    {
      "epoch": 1.5860805860805862,
      "grad_norm": 0.5427910089492798,
      "learning_rate": 9.578163771712159e-05,
      "loss": 0.3363,
      "step": 216
    },
    {
      "epoch": 1.5934065934065935,
      "grad_norm": 684.5294799804688,
      "learning_rate": 9.528535980148883e-05,
      "loss": 0.3832,
      "step": 217
    },
    {
      "epoch": 1.6007326007326008,
      "grad_norm": 0.6104606986045837,
      "learning_rate": 9.478908188585608e-05,
      "loss": 0.3727,
      "step": 218
    },
    {
      "epoch": 1.6080586080586081,
      "grad_norm": 0.6525624990463257,
      "learning_rate": 9.429280397022333e-05,
      "loss": 0.3832,
      "step": 219
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 0.556119441986084,
      "learning_rate": 9.379652605459058e-05,
      "loss": 0.3613,
      "step": 220
    },
    {
      "epoch": 1.6227106227106227,
      "grad_norm": 0.4977414309978485,
      "learning_rate": 9.330024813895782e-05,
      "loss": 0.253,
      "step": 221
    },
    {
      "epoch": 1.63003663003663,
      "grad_norm": 0.6264133453369141,
      "learning_rate": 9.280397022332506e-05,
      "loss": 0.2884,
      "step": 222
    },
    {
      "epoch": 1.6373626373626373,
      "grad_norm": 0.47159868478775024,
      "learning_rate": 9.230769230769232e-05,
      "loss": 0.2727,
      "step": 223
    },
    {
      "epoch": 1.6446886446886446,
      "grad_norm": 0.44854575395584106,
      "learning_rate": 9.181141439205956e-05,
      "loss": 0.2418,
      "step": 224
    },
    {
      "epoch": 1.652014652014652,
      "grad_norm": 0.5415360927581787,
      "learning_rate": 9.13151364764268e-05,
      "loss": 0.3157,
      "step": 225
    },
    {
      "epoch": 1.6593406593406592,
      "grad_norm": 0.5998497009277344,
      "learning_rate": 9.081885856079406e-05,
      "loss": 0.2883,
      "step": 226
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.535047709941864,
      "learning_rate": 9.032258064516129e-05,
      "loss": 0.3726,
      "step": 227
    },
    {
      "epoch": 1.673992673992674,
      "grad_norm": 0.5552490949630737,
      "learning_rate": 8.982630272952854e-05,
      "loss": 0.36,
      "step": 228
    },
    {
      "epoch": 1.6813186813186813,
      "grad_norm": 0.5908142924308777,
      "learning_rate": 8.933002481389578e-05,
      "loss": 0.2442,
      "step": 229
    },
    {
      "epoch": 1.6886446886446886,
      "grad_norm": 0.513377845287323,
      "learning_rate": 8.883374689826303e-05,
      "loss": 0.3137,
      "step": 230
    },
    {
      "epoch": 1.695970695970696,
      "grad_norm": 0.4955883026123047,
      "learning_rate": 8.833746898263028e-05,
      "loss": 0.2384,
      "step": 231
    },
    {
      "epoch": 1.7032967032967035,
      "grad_norm": 0.6351770758628845,
      "learning_rate": 8.784119106699753e-05,
      "loss": 0.3856,
      "step": 232
    },
    {
      "epoch": 1.7106227106227108,
      "grad_norm": 0.5647071003913879,
      "learning_rate": 8.734491315136477e-05,
      "loss": 0.3234,
      "step": 233
    },
    {
      "epoch": 1.717948717948718,
      "grad_norm": 0.45871034264564514,
      "learning_rate": 8.684863523573201e-05,
      "loss": 0.2187,
      "step": 234
    },
    {
      "epoch": 1.7252747252747254,
      "grad_norm": 0.5076830387115479,
      "learning_rate": 8.635235732009927e-05,
      "loss": 0.2325,
      "step": 235
    },
    {
      "epoch": 1.7326007326007327,
      "grad_norm": 0.43376025557518005,
      "learning_rate": 8.585607940446651e-05,
      "loss": 0.2696,
      "step": 236
    },
    {
      "epoch": 1.73992673992674,
      "grad_norm": 0.48224595189094543,
      "learning_rate": 8.535980148883375e-05,
      "loss": 0.2636,
      "step": 237
    },
    {
      "epoch": 1.7472527472527473,
      "grad_norm": 0.45744970440864563,
      "learning_rate": 8.486352357320099e-05,
      "loss": 0.1978,
      "step": 238
    },
    {
      "epoch": 1.7545787545787546,
      "grad_norm": 0.43513694405555725,
      "learning_rate": 8.436724565756824e-05,
      "loss": 0.2151,
      "step": 239
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.6038745641708374,
      "learning_rate": 8.387096774193549e-05,
      "loss": 0.3353,
      "step": 240
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 0.4076624810695648,
      "learning_rate": 8.337468982630273e-05,
      "loss": 0.1876,
      "step": 241
    },
    {
      "epoch": 1.7765567765567765,
      "grad_norm": 0.6100659966468811,
      "learning_rate": 8.287841191066999e-05,
      "loss": 0.3078,
      "step": 242
    },
    {
      "epoch": 1.7838827838827838,
      "grad_norm": 0.4874005615711212,
      "learning_rate": 8.238213399503722e-05,
      "loss": 0.2253,
      "step": 243
    },
    {
      "epoch": 1.791208791208791,
      "grad_norm": 0.4545210897922516,
      "learning_rate": 8.188585607940446e-05,
      "loss": 0.2069,
      "step": 244
    },
    {
      "epoch": 1.7985347985347986,
      "grad_norm": 0.41948699951171875,
      "learning_rate": 8.138957816377172e-05,
      "loss": 0.205,
      "step": 245
    },
    {
      "epoch": 1.8058608058608059,
      "grad_norm": 0.6355165839195251,
      "learning_rate": 8.089330024813896e-05,
      "loss": 0.3646,
      "step": 246
    },
    {
      "epoch": 1.8131868131868132,
      "grad_norm": 0.4434339702129364,
      "learning_rate": 8.039702233250622e-05,
      "loss": 0.244,
      "step": 247
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.5666961669921875,
      "learning_rate": 7.990074441687346e-05,
      "loss": 0.2903,
      "step": 248
    },
    {
      "epoch": 1.8278388278388278,
      "grad_norm": 876.4288940429688,
      "learning_rate": 7.94044665012407e-05,
      "loss": 0.2447,
      "step": 249
    },
    {
      "epoch": 1.8351648351648353,
      "grad_norm": 0.53396075963974,
      "learning_rate": 7.890818858560794e-05,
      "loss": 0.3011,
      "step": 250
    },
    {
      "epoch": 1.8424908424908426,
      "grad_norm": 0.5043901801109314,
      "learning_rate": 7.841191066997518e-05,
      "loss": 0.2429,
      "step": 251
    },
    {
      "epoch": 1.84981684981685,
      "grad_norm": 0.6087324023246765,
      "learning_rate": 7.791563275434244e-05,
      "loss": 0.343,
      "step": 252
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.6148578524589539,
      "learning_rate": 7.741935483870968e-05,
      "loss": 0.3691,
      "step": 253
    },
    {
      "epoch": 1.8644688644688645,
      "grad_norm": 0.48824596405029297,
      "learning_rate": 7.692307692307693e-05,
      "loss": 0.2193,
      "step": 254
    },
    {
      "epoch": 1.8717948717948718,
      "grad_norm": 0.5237699151039124,
      "learning_rate": 7.642679900744417e-05,
      "loss": 0.2117,
      "step": 255
    },
    {
      "epoch": 1.879120879120879,
      "grad_norm": 0.4046432077884674,
      "learning_rate": 7.593052109181141e-05,
      "loss": 0.179,
      "step": 256
    },
    {
      "epoch": 1.8864468864468864,
      "grad_norm": 0.5095109939575195,
      "learning_rate": 7.543424317617867e-05,
      "loss": 0.2422,
      "step": 257
    },
    {
      "epoch": 1.8937728937728937,
      "grad_norm": 0.5757139325141907,
      "learning_rate": 7.493796526054591e-05,
      "loss": 0.3176,
      "step": 258
    },
    {
      "epoch": 1.901098901098901,
      "grad_norm": 0.4575919806957245,
      "learning_rate": 7.444168734491315e-05,
      "loss": 0.2818,
      "step": 259
    },
    {
      "epoch": 1.9084249084249083,
      "grad_norm": 0.4597317576408386,
      "learning_rate": 7.39454094292804e-05,
      "loss": 0.2063,
      "step": 260
    },
    {
      "epoch": 1.9157509157509156,
      "grad_norm": 0.6048216819763184,
      "learning_rate": 7.344913151364765e-05,
      "loss": 0.3698,
      "step": 261
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 0.5493056178092957,
      "learning_rate": 7.295285359801489e-05,
      "loss": 0.3059,
      "step": 262
    },
    {
      "epoch": 1.9304029304029304,
      "grad_norm": 0.5007692575454712,
      "learning_rate": 7.245657568238213e-05,
      "loss": 0.3092,
      "step": 263
    },
    {
      "epoch": 1.9377289377289377,
      "grad_norm": 0.6907692551612854,
      "learning_rate": 7.196029776674939e-05,
      "loss": 0.2356,
      "step": 264
    },
    {
      "epoch": 1.945054945054945,
      "grad_norm": 17.368412017822266,
      "learning_rate": 7.146401985111662e-05,
      "loss": 0.249,
      "step": 265
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.48656171560287476,
      "learning_rate": 7.096774193548388e-05,
      "loss": 0.251,
      "step": 266
    },
    {
      "epoch": 1.9597069597069599,
      "grad_norm": 0.621129035949707,
      "learning_rate": 7.047146401985112e-05,
      "loss": 0.2997,
      "step": 267
    },
    {
      "epoch": 1.9670329670329672,
      "grad_norm": 0.6385814547538757,
      "learning_rate": 6.997518610421837e-05,
      "loss": 0.3879,
      "step": 268
    },
    {
      "epoch": 1.9743589743589745,
      "grad_norm": 0.5150391459465027,
      "learning_rate": 6.947890818858562e-05,
      "loss": 0.2629,
      "step": 269
    },
    {
      "epoch": 1.9816849816849818,
      "grad_norm": 0.5198465585708618,
      "learning_rate": 6.898263027295286e-05,
      "loss": 0.226,
      "step": 270
    },
    {
      "epoch": 1.989010989010989,
      "grad_norm": 0.6553750038146973,
      "learning_rate": 6.84863523573201e-05,
      "loss": 0.3218,
      "step": 271
    },
    {
      "epoch": 1.9963369963369964,
      "grad_norm": 0.4848983883857727,
      "learning_rate": 6.799007444168734e-05,
      "loss": 0.258,
      "step": 272
    },
    {
      "epoch": 2.0073260073260073,
      "grad_norm": 1.1008809804916382,
      "learning_rate": 6.74937965260546e-05,
      "loss": 0.4996,
      "step": 273
    },
    {
      "epoch": 2.0146520146520146,
      "grad_norm": 0.7856876850128174,
      "learning_rate": 6.699751861042184e-05,
      "loss": 0.1545,
      "step": 274
    },
    {
      "epoch": 2.021978021978022,
      "grad_norm": 0.43702244758605957,
      "learning_rate": 6.650124069478908e-05,
      "loss": 0.1686,
      "step": 275
    },
    {
      "epoch": 2.029304029304029,
      "grad_norm": 0.41352367401123047,
      "learning_rate": 6.600496277915633e-05,
      "loss": 0.1692,
      "step": 276
    },
    {
      "epoch": 2.0366300366300365,
      "grad_norm": 0.3963237404823303,
      "learning_rate": 6.550868486352357e-05,
      "loss": 0.1536,
      "step": 277
    },
    {
      "epoch": 2.043956043956044,
      "grad_norm": 0.3969533145427704,
      "learning_rate": 6.501240694789082e-05,
      "loss": 0.2114,
      "step": 278
    },
    {
      "epoch": 2.051282051282051,
      "grad_norm": 0.43230780959129333,
      "learning_rate": 6.451612903225807e-05,
      "loss": 0.1769,
      "step": 279
    },
    {
      "epoch": 2.0586080586080584,
      "grad_norm": 0.5180703997612,
      "learning_rate": 6.401985111662532e-05,
      "loss": 0.1927,
      "step": 280
    },
    {
      "epoch": 2.065934065934066,
      "grad_norm": 0.49213719367980957,
      "learning_rate": 6.352357320099257e-05,
      "loss": 0.1921,
      "step": 281
    },
    {
      "epoch": 2.0732600732600734,
      "grad_norm": 0.5932391881942749,
      "learning_rate": 6.30272952853598e-05,
      "loss": 0.3239,
      "step": 282
    },
    {
      "epoch": 2.0805860805860807,
      "grad_norm": 0.46651700139045715,
      "learning_rate": 6.253101736972705e-05,
      "loss": 0.1812,
      "step": 283
    },
    {
      "epoch": 2.087912087912088,
      "grad_norm": 0.44098910689353943,
      "learning_rate": 6.203473945409429e-05,
      "loss": 0.1628,
      "step": 284
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.447933554649353,
      "learning_rate": 6.153846153846155e-05,
      "loss": 0.1801,
      "step": 285
    },
    {
      "epoch": 2.1025641025641026,
      "grad_norm": 0.4540671408176422,
      "learning_rate": 6.104218362282879e-05,
      "loss": 0.1898,
      "step": 286
    },
    {
      "epoch": 2.10989010989011,
      "grad_norm": 0.5250617265701294,
      "learning_rate": 6.054590570719604e-05,
      "loss": 0.2074,
      "step": 287
    },
    {
      "epoch": 2.1172161172161172,
      "grad_norm": 0.48328253626823425,
      "learning_rate": 6.0049627791563276e-05,
      "loss": 0.1898,
      "step": 288
    },
    {
      "epoch": 2.1245421245421245,
      "grad_norm": 0.4517492353916168,
      "learning_rate": 5.955334987593052e-05,
      "loss": 0.1877,
      "step": 289
    },
    {
      "epoch": 2.131868131868132,
      "grad_norm": 73.52447509765625,
      "learning_rate": 5.9057071960297774e-05,
      "loss": 0.21,
      "step": 290
    },
    {
      "epoch": 2.139194139194139,
      "grad_norm": 0.41202643513679504,
      "learning_rate": 5.856079404466501e-05,
      "loss": 0.1624,
      "step": 291
    },
    {
      "epoch": 2.1465201465201464,
      "grad_norm": 0.5026519894599915,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 0.1935,
      "step": 292
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 0.507590115070343,
      "learning_rate": 5.756823821339951e-05,
      "loss": 0.2094,
      "step": 293
    },
    {
      "epoch": 2.161172161172161,
      "grad_norm": 0.545766294002533,
      "learning_rate": 5.7071960297766744e-05,
      "loss": 0.1844,
      "step": 294
    },
    {
      "epoch": 2.1684981684981683,
      "grad_norm": 0.4925268590450287,
      "learning_rate": 5.6575682382134e-05,
      "loss": 0.1918,
      "step": 295
    },
    {
      "epoch": 2.1758241758241756,
      "grad_norm": 0.5315663814544678,
      "learning_rate": 5.607940446650124e-05,
      "loss": 0.1648,
      "step": 296
    },
    {
      "epoch": 2.183150183150183,
      "grad_norm": 0.4719081223011017,
      "learning_rate": 5.558312655086849e-05,
      "loss": 0.1788,
      "step": 297
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.48282524943351746,
      "learning_rate": 5.5086848635235734e-05,
      "loss": 0.2228,
      "step": 298
    },
    {
      "epoch": 2.197802197802198,
      "grad_norm": 0.45744067430496216,
      "learning_rate": 5.459057071960298e-05,
      "loss": 0.1941,
      "step": 299
    },
    {
      "epoch": 2.2051282051282053,
      "grad_norm": 0.5002739429473877,
      "learning_rate": 5.4094292803970225e-05,
      "loss": 0.1765,
      "step": 300
    },
    {
      "epoch": 2.2124542124542126,
      "grad_norm": 0.40008580684661865,
      "learning_rate": 5.359801488833747e-05,
      "loss": 0.1766,
      "step": 301
    },
    {
      "epoch": 2.21978021978022,
      "grad_norm": 0.4376468062400818,
      "learning_rate": 5.310173697270472e-05,
      "loss": 0.1677,
      "step": 302
    },
    {
      "epoch": 2.227106227106227,
      "grad_norm": 0.4233366847038269,
      "learning_rate": 5.260545905707196e-05,
      "loss": 0.1943,
      "step": 303
    },
    {
      "epoch": 2.2344322344322345,
      "grad_norm": 0.44703349471092224,
      "learning_rate": 5.210918114143921e-05,
      "loss": 0.2081,
      "step": 304
    },
    {
      "epoch": 2.241758241758242,
      "grad_norm": 0.4660561680793762,
      "learning_rate": 5.161290322580645e-05,
      "loss": 0.1952,
      "step": 305
    },
    {
      "epoch": 2.249084249084249,
      "grad_norm": 0.39829328656196594,
      "learning_rate": 5.111662531017371e-05,
      "loss": 0.1431,
      "step": 306
    },
    {
      "epoch": 2.2564102564102564,
      "grad_norm": 0.5074213147163391,
      "learning_rate": 5.062034739454094e-05,
      "loss": 0.2274,
      "step": 307
    },
    {
      "epoch": 2.2637362637362637,
      "grad_norm": 0.5236841440200806,
      "learning_rate": 5.0124069478908185e-05,
      "loss": 0.1725,
      "step": 308
    },
    {
      "epoch": 2.271062271062271,
      "grad_norm": 0.40790361166000366,
      "learning_rate": 4.962779156327544e-05,
      "loss": 0.1514,
      "step": 309
    },
    {
      "epoch": 2.2783882783882783,
      "grad_norm": 0.38770103454589844,
      "learning_rate": 4.9131513647642677e-05,
      "loss": 0.1581,
      "step": 310
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.3932550847530365,
      "learning_rate": 4.8635235732009926e-05,
      "loss": 0.124,
      "step": 311
    },
    {
      "epoch": 2.293040293040293,
      "grad_norm": 0.4703129529953003,
      "learning_rate": 4.8138957816377175e-05,
      "loss": 0.1903,
      "step": 312
    },
    {
      "epoch": 2.3003663003663,
      "grad_norm": 0.4096028804779053,
      "learning_rate": 4.764267990074442e-05,
      "loss": 0.1715,
      "step": 313
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 0.4885402023792267,
      "learning_rate": 4.7146401985111667e-05,
      "loss": 0.1821,
      "step": 314
    },
    {
      "epoch": 2.315018315018315,
      "grad_norm": 0.4001680314540863,
      "learning_rate": 4.665012406947891e-05,
      "loss": 0.1088,
      "step": 315
    },
    {
      "epoch": 2.3223443223443225,
      "grad_norm": 0.4510026276111603,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.2091,
      "step": 316
    },
    {
      "epoch": 2.32967032967033,
      "grad_norm": 0.4708159267902374,
      "learning_rate": 4.56575682382134e-05,
      "loss": 0.2351,
      "step": 317
    },
    {
      "epoch": 2.336996336996337,
      "grad_norm": 6.576531410217285,
      "learning_rate": 4.516129032258064e-05,
      "loss": 0.1705,
      "step": 318
    },
    {
      "epoch": 2.3443223443223444,
      "grad_norm": 0.5455395579338074,
      "learning_rate": 4.466501240694789e-05,
      "loss": 0.2556,
      "step": 319
    },
    {
      "epoch": 2.3516483516483517,
      "grad_norm": 0.3699111044406891,
      "learning_rate": 4.416873449131514e-05,
      "loss": 0.1586,
      "step": 320
    },
    {
      "epoch": 2.358974358974359,
      "grad_norm": 0.430450975894928,
      "learning_rate": 4.3672456575682384e-05,
      "loss": 0.1752,
      "step": 321
    },
    {
      "epoch": 2.3663003663003663,
      "grad_norm": 0.39483755826950073,
      "learning_rate": 4.317617866004963e-05,
      "loss": 0.1329,
      "step": 322
    },
    {
      "epoch": 2.3736263736263736,
      "grad_norm": 0.4844495356082916,
      "learning_rate": 4.2679900744416875e-05,
      "loss": 0.1823,
      "step": 323
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.5492668151855469,
      "learning_rate": 4.218362282878412e-05,
      "loss": 0.2068,
      "step": 324
    },
    {
      "epoch": 2.3882783882783882,
      "grad_norm": 0.3951610326766968,
      "learning_rate": 4.168734491315137e-05,
      "loss": 0.148,
      "step": 325
    },
    {
      "epoch": 2.3956043956043955,
      "grad_norm": 0.4531758427619934,
      "learning_rate": 4.119106699751861e-05,
      "loss": 0.2048,
      "step": 326
    },
    {
      "epoch": 2.402930402930403,
      "grad_norm": 0.43211448192596436,
      "learning_rate": 4.069478908188586e-05,
      "loss": 0.16,
      "step": 327
    },
    {
      "epoch": 2.41025641025641,
      "grad_norm": 0.4329107105731964,
      "learning_rate": 4.019851116625311e-05,
      "loss": 0.1511,
      "step": 328
    },
    {
      "epoch": 2.4175824175824174,
      "grad_norm": 0.4688233733177185,
      "learning_rate": 3.970223325062035e-05,
      "loss": 0.17,
      "step": 329
    },
    {
      "epoch": 2.4249084249084247,
      "grad_norm": 0.4433632493019104,
      "learning_rate": 3.920595533498759e-05,
      "loss": 0.1709,
      "step": 330
    },
    {
      "epoch": 2.4322344322344325,
      "grad_norm": 0.486644446849823,
      "learning_rate": 3.870967741935484e-05,
      "loss": 0.2249,
      "step": 331
    },
    {
      "epoch": 2.4395604395604398,
      "grad_norm": 0.45183196663856506,
      "learning_rate": 3.8213399503722084e-05,
      "loss": 0.1677,
      "step": 332
    },
    {
      "epoch": 2.446886446886447,
      "grad_norm": 0.41756534576416016,
      "learning_rate": 3.771712158808933e-05,
      "loss": 0.1576,
      "step": 333
    },
    {
      "epoch": 2.4542124542124544,
      "grad_norm": 0.49159741401672363,
      "learning_rate": 3.7220843672456576e-05,
      "loss": 0.2248,
      "step": 334
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 0.5216251611709595,
      "learning_rate": 3.6724565756823825e-05,
      "loss": 0.2313,
      "step": 335
    },
    {
      "epoch": 2.468864468864469,
      "grad_norm": 0.4868141710758209,
      "learning_rate": 3.622828784119107e-05,
      "loss": 0.1729,
      "step": 336
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.42846694588661194,
      "learning_rate": 3.573200992555831e-05,
      "loss": 0.159,
      "step": 337
    },
    {
      "epoch": 2.4835164835164836,
      "grad_norm": 0.45092037320137024,
      "learning_rate": 3.523573200992556e-05,
      "loss": 0.1504,
      "step": 338
    },
    {
      "epoch": 2.490842490842491,
      "grad_norm": 0.5391306281089783,
      "learning_rate": 3.473945409429281e-05,
      "loss": 0.2714,
      "step": 339
    },
    {
      "epoch": 2.498168498168498,
      "grad_norm": 0.3963892459869385,
      "learning_rate": 3.424317617866005e-05,
      "loss": 0.1179,
      "step": 340
    },
    {
      "epoch": 2.5054945054945055,
      "grad_norm": 0.4640357196331024,
      "learning_rate": 3.37468982630273e-05,
      "loss": 0.2013,
      "step": 341
    },
    {
      "epoch": 2.5128205128205128,
      "grad_norm": 0.3701193630695343,
      "learning_rate": 3.325062034739454e-05,
      "loss": 0.1619,
      "step": 342
    },
    {
      "epoch": 2.52014652014652,
      "grad_norm": 0.47007760405540466,
      "learning_rate": 3.2754342431761784e-05,
      "loss": 0.2209,
      "step": 343
    },
    {
      "epoch": 2.5274725274725274,
      "grad_norm": 0.4288915693759918,
      "learning_rate": 3.2258064516129034e-05,
      "loss": 0.1801,
      "step": 344
    },
    {
      "epoch": 2.5347985347985347,
      "grad_norm": 0.3867497444152832,
      "learning_rate": 3.176178660049628e-05,
      "loss": 0.2025,
      "step": 345
    },
    {
      "epoch": 2.542124542124542,
      "grad_norm": 0.49012291431427,
      "learning_rate": 3.1265508684863525e-05,
      "loss": 0.1851,
      "step": 346
    },
    {
      "epoch": 2.5494505494505493,
      "grad_norm": 0.8456919193267822,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 0.2179,
      "step": 347
    },
    {
      "epoch": 2.5567765567765566,
      "grad_norm": 0.3846389353275299,
      "learning_rate": 3.027295285359802e-05,
      "loss": 0.1342,
      "step": 348
    },
    {
      "epoch": 2.564102564102564,
      "grad_norm": 0.5923460125923157,
      "learning_rate": 2.977667493796526e-05,
      "loss": 0.23,
      "step": 349
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.3573910892009735,
      "learning_rate": 2.9280397022332505e-05,
      "loss": 0.143,
      "step": 350
    },
    {
      "epoch": 2.578754578754579,
      "grad_norm": 0.3937934935092926,
      "learning_rate": 2.8784119106699754e-05,
      "loss": 0.1259,
      "step": 351
    },
    {
      "epoch": 2.586080586080586,
      "grad_norm": 0.4373726546764374,
      "learning_rate": 2.8287841191067e-05,
      "loss": 0.138,
      "step": 352
    },
    {
      "epoch": 2.5934065934065935,
      "grad_norm": 0.4079678952693939,
      "learning_rate": 2.7791563275434246e-05,
      "loss": 0.1752,
      "step": 353
    },
    {
      "epoch": 2.600732600732601,
      "grad_norm": 0.44363072514533997,
      "learning_rate": 2.729528535980149e-05,
      "loss": 0.2063,
      "step": 354
    },
    {
      "epoch": 2.608058608058608,
      "grad_norm": 0.5644967555999756,
      "learning_rate": 2.6799007444168734e-05,
      "loss": 0.2355,
      "step": 355
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 0.41904690861701965,
      "learning_rate": 2.630272952853598e-05,
      "loss": 0.183,
      "step": 356
    },
    {
      "epoch": 2.6227106227106227,
      "grad_norm": 0.5036385655403137,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 0.2149,
      "step": 357
    },
    {
      "epoch": 2.63003663003663,
      "grad_norm": 0.39802417159080505,
      "learning_rate": 2.531017369727047e-05,
      "loss": 0.133,
      "step": 358
    },
    {
      "epoch": 2.6373626373626373,
      "grad_norm": 0.3897470235824585,
      "learning_rate": 2.481389578163772e-05,
      "loss": 0.1613,
      "step": 359
    },
    {
      "epoch": 2.6446886446886446,
      "grad_norm": 0.4407351016998291,
      "learning_rate": 2.4317617866004963e-05,
      "loss": 0.2141,
      "step": 360
    },
    {
      "epoch": 2.652014652014652,
      "grad_norm": 0.3955513834953308,
      "learning_rate": 2.382133995037221e-05,
      "loss": 0.1667,
      "step": 361
    },
    {
      "epoch": 2.659340659340659,
      "grad_norm": 0.39339181780815125,
      "learning_rate": 2.3325062034739454e-05,
      "loss": 0.1423,
      "step": 362
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.36414459347724915,
      "learning_rate": 2.28287841191067e-05,
      "loss": 0.1753,
      "step": 363
    },
    {
      "epoch": 2.6739926739926743,
      "grad_norm": 0.5247775316238403,
      "learning_rate": 2.2332506203473946e-05,
      "loss": 0.2125,
      "step": 364
    },
    {
      "epoch": 2.6813186813186816,
      "grad_norm": 0.5289739370346069,
      "learning_rate": 2.1836228287841192e-05,
      "loss": 0.2403,
      "step": 365
    },
    {
      "epoch": 2.688644688644689,
      "grad_norm": 0.3878862261772156,
      "learning_rate": 2.1339950372208438e-05,
      "loss": 0.1508,
      "step": 366
    },
    {
      "epoch": 2.695970695970696,
      "grad_norm": 0.48352301120758057,
      "learning_rate": 2.0843672456575683e-05,
      "loss": 0.2203,
      "step": 367
    },
    {
      "epoch": 2.7032967032967035,
      "grad_norm": 0.4097227454185486,
      "learning_rate": 2.034739454094293e-05,
      "loss": 0.1679,
      "step": 368
    },
    {
      "epoch": 2.7106227106227108,
      "grad_norm": 0.4986218214035034,
      "learning_rate": 1.9851116625310175e-05,
      "loss": 0.2035,
      "step": 369
    },
    {
      "epoch": 2.717948717948718,
      "grad_norm": 0.48403626680374146,
      "learning_rate": 1.935483870967742e-05,
      "loss": 0.2194,
      "step": 370
    },
    {
      "epoch": 2.7252747252747254,
      "grad_norm": 0.4938884973526001,
      "learning_rate": 1.8858560794044667e-05,
      "loss": 0.1853,
      "step": 371
    },
    {
      "epoch": 2.7326007326007327,
      "grad_norm": 0.4570513665676117,
      "learning_rate": 1.8362282878411912e-05,
      "loss": 0.19,
      "step": 372
    },
    {
      "epoch": 2.73992673992674,
      "grad_norm": 0.4826517701148987,
      "learning_rate": 1.7866004962779155e-05,
      "loss": 0.2007,
      "step": 373
    },
    {
      "epoch": 2.7472527472527473,
      "grad_norm": 0.45772477984428406,
      "learning_rate": 1.7369727047146404e-05,
      "loss": 0.1798,
      "step": 374
    },
    {
      "epoch": 2.7545787545787546,
      "grad_norm": 0.4125435948371887,
      "learning_rate": 1.687344913151365e-05,
      "loss": 0.1573,
      "step": 375
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.3910561501979828,
      "learning_rate": 1.6377171215880892e-05,
      "loss": 0.1446,
      "step": 376
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 0.44632434844970703,
      "learning_rate": 1.588089330024814e-05,
      "loss": 0.194,
      "step": 377
    },
    {
      "epoch": 2.7765567765567765,
      "grad_norm": 0.40176504850387573,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.1652,
      "step": 378
    },
    {
      "epoch": 2.7838827838827838,
      "grad_norm": 0.39241161942481995,
      "learning_rate": 1.488833746898263e-05,
      "loss": 0.1329,
      "step": 379
    },
    {
      "epoch": 2.791208791208791,
      "grad_norm": 0.517867386341095,
      "learning_rate": 1.4392059553349877e-05,
      "loss": 0.2223,
      "step": 380
    },
    {
      "epoch": 2.7985347985347984,
      "grad_norm": 0.39505913853645325,
      "learning_rate": 1.3895781637717123e-05,
      "loss": 0.1386,
      "step": 381
    },
    {
      "epoch": 2.8058608058608057,
      "grad_norm": 0.3740110397338867,
      "learning_rate": 1.3399503722084367e-05,
      "loss": 0.1399,
      "step": 382
    },
    {
      "epoch": 2.813186813186813,
      "grad_norm": 0.46499064564704895,
      "learning_rate": 1.2903225806451613e-05,
      "loss": 0.172,
      "step": 383
    },
    {
      "epoch": 2.8205128205128203,
      "grad_norm": 0.46060433983802795,
      "learning_rate": 1.240694789081886e-05,
      "loss": 0.1922,
      "step": 384
    },
    {
      "epoch": 2.8278388278388276,
      "grad_norm": 0.4734959602355957,
      "learning_rate": 1.1910669975186104e-05,
      "loss": 0.2155,
      "step": 385
    },
    {
      "epoch": 2.8351648351648353,
      "grad_norm": 0.38484224677085876,
      "learning_rate": 1.141439205955335e-05,
      "loss": 0.1607,
      "step": 386
    },
    {
      "epoch": 2.8424908424908426,
      "grad_norm": 0.4356207549571991,
      "learning_rate": 1.0918114143920596e-05,
      "loss": 0.2008,
      "step": 387
    },
    {
      "epoch": 2.84981684981685,
      "grad_norm": 0.4469323456287384,
      "learning_rate": 1.0421836228287842e-05,
      "loss": 0.1562,
      "step": 388
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.42687585949897766,
      "learning_rate": 9.925558312655088e-06,
      "loss": 0.1684,
      "step": 389
    },
    {
      "epoch": 2.8644688644688645,
      "grad_norm": 0.39251017570495605,
      "learning_rate": 9.429280397022333e-06,
      "loss": 0.1605,
      "step": 390
    },
    {
      "epoch": 2.871794871794872,
      "grad_norm": 0.4703003764152527,
      "learning_rate": 8.933002481389577e-06,
      "loss": 0.2012,
      "step": 391
    },
    {
      "epoch": 2.879120879120879,
      "grad_norm": 0.37344861030578613,
      "learning_rate": 8.436724565756825e-06,
      "loss": 0.1562,
      "step": 392
    },
    {
      "epoch": 2.8864468864468864,
      "grad_norm": 0.48176804184913635,
      "learning_rate": 7.94044665012407e-06,
      "loss": 0.2216,
      "step": 393
    },
    {
      "epoch": 2.8937728937728937,
      "grad_norm": 0.3984513282775879,
      "learning_rate": 7.444168734491315e-06,
      "loss": 0.1558,
      "step": 394
    },
    {
      "epoch": 2.901098901098901,
      "grad_norm": 0.4182524085044861,
      "learning_rate": 6.9478908188585614e-06,
      "loss": 0.1993,
      "step": 395
    },
    {
      "epoch": 2.9084249084249083,
      "grad_norm": 0.5200336575508118,
      "learning_rate": 6.451612903225806e-06,
      "loss": 0.1756,
      "step": 396
    },
    {
      "epoch": 2.9157509157509156,
      "grad_norm": 0.4554533064365387,
      "learning_rate": 5.955334987593052e-06,
      "loss": 0.2187,
      "step": 397
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 0.3989841341972351,
      "learning_rate": 5.459057071960298e-06,
      "loss": 0.153,
      "step": 398
    },
    {
      "epoch": 2.9304029304029307,
      "grad_norm": 0.4027474522590637,
      "learning_rate": 4.962779156327544e-06,
      "loss": 0.1197,
      "step": 399
    },
    {
      "epoch": 2.937728937728938,
      "grad_norm": 0.4116552472114563,
      "learning_rate": 4.466501240694789e-06,
      "loss": 0.1678,
      "step": 400
    },
    {
      "epoch": 2.9450549450549453,
      "grad_norm": 0.34495168924331665,
      "learning_rate": 3.970223325062035e-06,
      "loss": 0.1276,
      "step": 401
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.4266985058784485,
      "learning_rate": 3.4739454094292807e-06,
      "loss": 0.1568,
      "step": 402
    },
    {
      "epoch": 2.95970695970696,
      "grad_norm": 0.47068601846694946,
      "learning_rate": 2.977667493796526e-06,
      "loss": 0.169,
      "step": 403
    },
    {
      "epoch": 2.967032967032967,
      "grad_norm": 0.39090344309806824,
      "learning_rate": 2.481389578163772e-06,
      "loss": 0.1704,
      "step": 404
    },
    {
      "epoch": 2.9743589743589745,
      "grad_norm": 0.42562994360923767,
      "learning_rate": 1.9851116625310177e-06,
      "loss": 0.1669,
      "step": 405
    },
    {
      "epoch": 2.9816849816849818,
      "grad_norm": 0.4056636393070221,
      "learning_rate": 1.488833746898263e-06,
      "loss": 0.1607,
      "step": 406
    },
    {
      "epoch": 2.989010989010989,
      "grad_norm": 287.012939453125,
      "learning_rate": 9.925558312655088e-07,
      "loss": 0.2322,
      "step": 407
    },
    {
      "epoch": 2.9963369963369964,
      "grad_norm": 0.5116565227508545,
      "learning_rate": 4.962779156327544e-07,
      "loss": 0.2333,
      "step": 408
    }
  ],
  "logging_steps": 1,
  "max_steps": 408,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.4831398575761e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
